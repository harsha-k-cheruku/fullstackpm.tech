# Validation Checklist for Claude Code

## Purpose

When the user sends back BUILD_02 through BUILD_08 generated by another LLM, use this checklist to validate each file before approving it for developer use.

## For Each BUILD File

### 1. Structure Validation (Pass/Fail)

- [ ] Has all 8 sections in correct order
- [ ] Section 1: Project Overview (standard boilerplate)
- [ ] Section 2: Tech Stack (table with mandatory constraints)
- [ ] Section 3: Design System (colors + typography pasted inline)
- [ ] Section 4: Coding Conventions (Python, Jinja2, HTMX rules)
- [ ] Section 5: The Task (unique, detailed spec)
- [ ] Section 6: Expected Output (file list)
- [ ] Section 7: Project Structure (ASCII tree)
- [ ] Section 8: Acceptance Test (5-12 concrete tests)

### 2. Self-Containment Check (Pass/Fail)

- [ ] Color system is pasted inline (not "see custom.css")
- [ ] Typography classes are pasted inline
- [ ] All referenced existing files are included with FULL text
- [ ] Data models are defined inline (not "see Task 1")
- [ ] Sample data is included inline
- [ ] No dangling references to other tasks or external docs

### 3. Tech Stack Compliance (Pass/Fail)

- [ ] Uses FastAPI (not Flask, Django, etc.)
- [ ] Uses Jinja2 for templates (not React, Vue, etc.)
- [ ] Uses Tailwind CSS via CDN (not custom CSS, SCSS, Bootstrap)
- [ ] Uses HTMX for interactivity (not jQuery, vanilla JS frameworks)
- [ ] Uses SQLite + SQLAlchemy async (not MongoDB, Prisma, etc.)
- [ ] Uses Claude API (not OpenAI only, no local models)
- [ ] Uses Chart.js for visualizations (not D3, Recharts, etc.)
- [ ] No npm/node/webpack/build tools mentioned

### 4. Code Quality (Score 1-10)

**Scoring rubric:**

**10/10 - Exceptional:**
- All async/await patterns correct
- Type hints on all functions
- Proper error handling with try/except
- Pydantic models with validators
- Complete file content (no "..." or "# rest of file")
- Production-ready patterns (logging, retries, timeouts)

**8-9/10 - Excellent:**
- Mostly async, minor sync slip-ups
- Type hints on 90%+ of functions
- Error handling present but could be more robust
- Pydantic models without validators
- Complete file content
- Good patterns, not quite production-ready

**6-7/10 - Good:**
- Mix of async and sync (inconsistent)
- Type hints on 70%+ of functions
- Basic error handling (some try/except)
- Plain dicts instead of Pydantic models
- Complete file content
- Works but needs polish

**4-5/10 - Acceptable:**
- Mostly sync, minimal async
- Type hints on 50%+ of functions
- Minimal error handling
- No Pydantic models
- Some "..." or abbreviated sections
- Works but not production-ready

**1-3/10 - Needs Rework:**
- All sync (no async)
- Few or no type hints
- No error handling
- Lots of "..." or "# TODO" comments
- Missing critical functionality
- Won't work without major fixes

### 5. Acceptance Tests (Score 1-10)

**Scoring rubric:**

**10/10:**
- 8-12 concrete tests
- Each has exact command + expected output
- Tests cover happy path + edge cases
- Tests are independent (runnable in any order)
- Mix of curl, browser checks, and code assertions

**8-9/10:**
- 6-8 tests
- Commands + expected outputs present
- Mostly happy path, some edge cases
- Tests are independent
- Good mix of test types

**6-7/10:**
- 4-6 tests
- Commands present, expected outputs vague
- Only happy path
- Some test dependencies
- Limited test coverage

**4-5/10:**
- 2-4 tests
- Vague commands ("check the page works")
- No expected outputs
- Tests depend on each other
- Minimal coverage

**1-3/10:**
- 0-2 tests
- No concrete commands
- No expected outputs
- Not actionable

### 6. Completeness (Pass/Fail)

- [ ] All files in "Expected Output" section are included
- [ ] Each file has complete content (no "... rest of implementation")
- [ ] Code samples are full functions/classes (not snippets)
- [ ] Templates have complete HTML structure
- [ ] No placeholders like "TODO", "# Your code here", etc.

### 7. Design System Compliance (Pass/Fail)

- [ ] All colors use CSS variables (no hardcoded hex)
- [ ] Typography uses .text-h1, .text-body, etc. classes
- [ ] Geist Sans for UI text, JetBrains Mono for code
- [ ] No Tailwind dark: prefix (design system handles dark mode)
- [ ] Heroicons are inline SVG (not icon fonts or external images)

### 8. HTMX Patterns (Pass/Fail, if applicable)

- [ ] Uses hx-get for GET requests (not hx-post for reads)
- [ ] Uses hx-target to specify swap element
- [ ] Uses hx-swap to control insertion (innerHTML, outerHTML, etc.)
- [ ] Uses hx-indicator for loading states
- [ ] Partial templates do NOT extend base.html
- [ ] No duplicate HTMX script tags (already in base.html)

## Validation Report Template

For each BUILD file, provide this report:

```markdown
## BUILD_0X_[NAME].md

**Structure:** ✅ Pass / ❌ Fail
**Self-Contained:** ✅ Pass / ❌ Fail
**Tech Stack:** ✅ Pass / ❌ Fail
**Code Quality:** [Score]/10
**Acceptance Tests:** [Score]/10
**Completeness:** ✅ Pass / ❌ Fail
**Design System:** ✅ Pass / ❌ Fail
**HTMX Patterns:** ✅ Pass / ❌ Fail / N/A

**Overall:** ✅ Ready to Use / ⚠️ Needs Minor Fixes / ❌ Needs Rework

**Issues Found:**
1. [Specific issue with line reference if possible]
2. [Another issue]

**Recommended Fixes:**
1. [What to change]
2. [How to fix it]

**Estimated Fix Effort:** [Trivial / Minor / Moderate / Major]
```

## Overall Project Validation

After validating all 7 files, provide a summary:

```markdown
## PM Interview Coach Build Tasks - Summary

**Files Validated:** 7/7

**Ready to Use:** [X]/7
**Needs Minor Fixes:** [Y]/7
**Needs Rework:** [Z]/7

**Average Code Quality:** [Score]/10
**Average Test Quality:** [Score]/10

**Critical Issues:** [Count]
**Blocking Issues:** [Count]
**Nice-to-Have Improvements:** [Count]

**Recommended Next Steps:**
1. [What to do first]
2. [What to do next]

**Estimated Developer Time to Implement:**
- If all files are used as-is: [X] hours
- After applying recommended fixes: [Y] hours

**LLM Comparison Notes:**
[If comparing outputs from multiple LLMs, note which one did better on which tasks]
```

## Red Flags to Watch For

These are instant "Needs Rework" signals:

1. **Framework contamination:** Mentions React, Next.js, npm install, webpack
2. **Database switching:** Uses MongoDB, Prisma, Firebase instead of SQLite + SQLAlchemy
3. **Missing async:** All route handlers are sync `def` instead of `async def`
4. **Hardcoded colors:** Templates use #FFFFFF instead of var(--color-bg-primary)
5. **External references:** Says "see the design system file" instead of pasting inline
6. **Incomplete code:** Has "..." or "# implementation omitted" in critical sections
7. **Wrong AI provider:** Uses OpenAI only, doesn't mention Claude API
8. **CSS frameworks:** Adds Bootstrap, Material-UI, custom SCSS
9. **Client-side rendering:** Suggests SPA patterns, client-side routing
10. **Missing sections:** Skips Section 3 (Design System) or Section 8 (Acceptance Tests)

## Quality Thresholds

**Minimum acceptable scores:**
- Code Quality: 6/10 (can be polished later)
- Acceptance Tests: 6/10 (can add more tests later)

**All Pass/Fail checks must pass** — no exceptions.

**If 5+ files score 8+ in both quality metrics:** This LLM is excellent at following specs.
**If 5+ files score 6-7:** This LLM is decent but needs human review.
**If 3+ files score below 6:** Recommend trying a different LLM.

## After Validation

1. Share the validation report with the user
2. If files need fixes, provide specific edit commands
3. If files are ready, tell the user which tasks can be parallelized
4. Suggest which LLM to use for the actual code generation (based on this performance)
